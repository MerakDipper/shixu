{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "shixu.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg7ioxFP93Ma"
      },
      "source": [
        "#-*- coding: utf-8 -*-\n",
        "import glob\n",
        "import json"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASlokxKR9um3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcb03eac-2349-4166-80ed-36a5349e6ebb"
      },
      "source": [
        "!git clone https://github.com/chinese-poetry/chinese-poetry"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'chinese-poetry'...\n",
            "remote: Enumerating objects: 10, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 5801 (delta 1), reused 3 (delta 1), pack-reused 5791\u001b[K\n",
            "Receiving objects: 100% (5801/5801), 185.66 MiB | 31.65 MiB/s, done.\n",
            "Resolving deltas: 100% (4503/4503), done.\n",
            "Checking out files: 100% (1372/1372), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nbfe7EtiB3Qe",
        "outputId": "13e4a581-fb10-420f-97ea-f4ea43a34f81"
      },
      "source": [
        "!pip install opencc"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting opencc\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/b4/24e677e135df130fc6989929dc3990a1ae19948daf28beb8f910b4f7b671/OpenCC-1.1.1.post1-py2.py3-none-manylinux1_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 9.2MB/s \n",
            "\u001b[?25hInstalling collected packages: opencc\n",
            "Successfully installed opencc-1.1.1.post1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRPYu6-W958-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14cc8f21-e274-45e8-ad80-bf4d482cbeca"
      },
      "source": [
        "# 词 is located under ~/chinese-poetry/ci\n",
        "# 诗 is located under ~/chinese-poetry/json\n",
        "# poet name could be in 繁體 or 简体 so its better to search for both\n",
        "import opencc\n",
        "converter_t2s = opencc.OpenCC('t2s.json')\n",
        "converter_s2t = opencc.OpenCC('s2t.json')\n",
        "# 摘出某个诗人的诗\n",
        "poet_name_simple = \"王安石\"\n",
        "poet_name = [poet_name_simple, converter_s2t.convert(poet_name_simple)]\n",
        "dynasty = \"song\" # choice from [\"tang\", \"song\"]\n",
        "shiciji_file_name = poet_name_simple\n",
        "shiciji5 = []\n",
        "shiciji7 = []\n",
        "yan = set([])\n",
        "for p_name in poet_name:\n",
        "    files = glob.glob(\"./chinese-poetry/json/poet.{}*.json\".format(dynasty))\n",
        "    #files1 = glob.glob(\"./chinese-poetry/ci/*.json\")\n",
        "    #files.extend(files1)\n",
        "    for file in files:\n",
        "        with open(file) as fi:\n",
        "            fi_json = json.load(fi)\n",
        "            for poem in fi_json:\n",
        "                if 'author' in poem and poem[\"author\"] == p_name:\n",
        "                    sen_len = len(poem[\"paragraphs\"][0])\n",
        "                    poem_str = converter_t2s.convert(poem['title'])+\":\"+converter_t2s.convert(\"\".join(poem[\"paragraphs\"]))\n",
        "                    if sen_len in [6, 12]: #五言\n",
        "                      shiciji5.append(poem_str)\n",
        "                    elif sen_len in [8, 16]: #七言\n",
        "                      shiciji7.append(poem_str)\n",
        "\n",
        "print(len(shiciji5))\n",
        "print(len(shiciji7))\n",
        "fo5 = open(shiciji_file_name+\"5\", 'w') #五言\n",
        "fo7 = open(shiciji_file_name+\"7\", 'w') #七言\n",
        "fo = open(shiciji_file_name, 'w') #所有\n",
        "for item in shiciji5:\n",
        "    fo5.write(item)\n",
        "    fo5.write(\"\\n\")\n",
        "    fo.write(item)\n",
        "    fo.write(\"\\n\")\n",
        "for item in shiciji7:\n",
        "    fo7.write(item)\n",
        "    fo7.write(\"\\n\")\n",
        "    fo.write(item)\n",
        "    fo.write(\"\\n\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1198\n",
            "2236\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRtfbtJlDMoV",
        "outputId": "1bab2874-74f9-4e31-ade1-e1192bdb9970"
      },
      "source": [
        "!pip install keras"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.4.3)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_3eWFXcAEbc"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.utils import np_utils\n",
        "input_file = shiciji_file_name #选择生成的训练集（五言或者7言或者所有）\n",
        "with open(input_file) as f:\n",
        "    raw_text = f.read()\n",
        "lines = raw_text.split(\"\\n\")[:-1]\n",
        "poem_text = [i.split(':')[1] for i in lines]\n",
        "char_list = [re.findall('[\\x80-\\xff]{3}|[\\w\\W]', s) for s in poem_text]\n",
        "all_words = []\n",
        "for i in char_list:\n",
        "    all_words.extend(i)\n",
        "word_dataframe = pd.DataFrame(pd.Series(all_words).value_counts())\n",
        "word_dataframe['id']=list(range(1,len(word_dataframe)+1))\n",
        "word_index_dict = word_dataframe['id'].to_dict()\n",
        "index_dict = {}\n",
        "for k in word_index_dict:\n",
        "    index_dict.update({word_index_dict[k]:k})\n",
        "seq_len = 2 # 输入长度\n",
        "rotate_len = 1 # 间隔长度\n",
        "dataX = []\n",
        "dataY = []\n",
        "for sentence in char_list:\n",
        "  for i in range(0, len(sentence) - seq_len, rotate_len):\n",
        "      seq_in = sentence[i : i + seq_len]\n",
        "      seq_out = sentence[i + seq_len]\n",
        "      dataX.append([word_index_dict[x] for x in seq_in])\n",
        "      dataY.append(word_index_dict[seq_out])\n",
        "X = np.array(dataX)\n",
        "y = np_utils.to_categorical(np.array(dataY))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClaFrKanMXHT",
        "outputId": "3f709ca5-c1ed-4002-cd37-0a47577e86a7"
      },
      "source": [
        "for i in range(7):\n",
        "  print([index_dict[word_index] for word_index in dataX[i]])\n",
        "  print(index_dict[dataY[i]])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['去', '序']\n",
            "三\n",
            "['序', '三']\n",
            "朝\n",
            "['三', '朝']\n",
            "圣\n",
            "['朝', '圣']\n",
            "，\n",
            "['圣', '，']\n",
            "行\n",
            "['，', '行']\n",
            "崩\n",
            "['行', '崩']\n",
            "万\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNfTzkupMjb8"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TUe2IJg_Ljx"
      },
      "source": [
        "from keras import Sequential\n",
        "from keras.layers import Embedding, GRU, LSTM, Dense, Activation\n",
        "import os\n",
        "\n",
        "def make_model():\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(len(word_dataframe)+1, 512))\n",
        "  #model.add(LSTM(512, return_sequences = True))\n",
        "  model.add(GRU(512))\n",
        "  model.add(Dense(y.shape[1]))\n",
        "  model.add(Activation('softmax'))\n",
        "  return model\n",
        "\n",
        "# tf.keras.backend.clear_session()\n",
        "\n",
        "# resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "# tf.config.experimental_connect_to_cluster(resolver)\n",
        "# # This is the TPU initialization code that has to be at the beginning.\n",
        "# tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "# print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "\n",
        "# strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "\n",
        "# with strategy.scope():\n",
        "training_model = make_model()\n",
        "training_model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fV5eKNVBEymp",
        "outputId": "3e9cd5a8-5aaf-4138-ff8a-d2f74c496505"
      },
      "source": [
        "# define the checkpoint\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import time;\n",
        "ts = int(time.time())\n",
        "checkpoint_dir = \"./\"+input_file+\"-checkpoints-\"+str(ts)\n",
        "os.makedirs(checkpoint_dir)\n",
        "#filepath=checkpoint_dir+\"/weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(checkpoint_dir, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "# fit the model\n",
        "training_model.fit(X, y, epochs=25, batch_size=256, callbacks=callbacks_list)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "792/792 [==============================] - ETA: 0s - loss: 6.4713\n",
            "Epoch 00001: loss improved from inf to 6.47133, saving model to ./王安石-checkpoints-1607977655\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: ./王安石-checkpoints-1607977655/assets\n",
            "792/792 [==============================] - 25s 32ms/step - loss: 6.4713\n",
            "Epoch 2/25\n",
            "792/792 [==============================] - ETA: 0s - loss: 5.6389\n",
            "Epoch 00002: loss improved from 6.47133 to 5.63887, saving model to ./王安石-checkpoints-1607977655\n",
            "INFO:tensorflow:Assets written to: ./王安石-checkpoints-1607977655/assets\n",
            "792/792 [==============================] - 26s 32ms/step - loss: 5.6389\n",
            "Epoch 3/25\n",
            "791/792 [============================>.] - ETA: 0s - loss: 4.7609\n",
            "Epoch 00003: loss improved from 5.63887 to 4.76061, saving model to ./王安石-checkpoints-1607977655\n",
            "INFO:tensorflow:Assets written to: ./王安石-checkpoints-1607977655/assets\n",
            "792/792 [==============================] - 25s 32ms/step - loss: 4.7606\n",
            "Epoch 4/25\n",
            "792/792 [==============================] - ETA: 0s - loss: 3.8528\n",
            "Epoch 00004: loss improved from 4.76061 to 3.85278, saving model to ./王安石-checkpoints-1607977655\n",
            "INFO:tensorflow:Assets written to: ./王安石-checkpoints-1607977655/assets\n",
            "792/792 [==============================] - 26s 33ms/step - loss: 3.8528\n",
            "Epoch 5/25\n",
            "791/792 [============================>.] - ETA: 0s - loss: 3.1387\n",
            "Epoch 00005: loss improved from 3.85278 to 3.13871, saving model to ./王安石-checkpoints-1607977655\n",
            "INFO:tensorflow:Assets written to: ./王安石-checkpoints-1607977655/assets\n",
            "792/792 [==============================] - 25s 32ms/step - loss: 3.1387\n",
            "Epoch 6/25\n",
            "791/792 [============================>.] - ETA: 0s - loss: 2.6294\n",
            "Epoch 00006: loss improved from 3.13871 to 2.62932, saving model to ./王安石-checkpoints-1607977655\n",
            "INFO:tensorflow:Assets written to: ./王安石-checkpoints-1607977655/assets\n",
            "792/792 [==============================] - 25s 32ms/step - loss: 2.6293\n",
            "Epoch 7/25\n",
            "791/792 [============================>.] - ETA: 0s - loss: 2.2734\n",
            "Epoch 00007: loss improved from 2.62932 to 2.27327, saving model to ./王安石-checkpoints-1607977655\n",
            "INFO:tensorflow:Assets written to: ./王安石-checkpoints-1607977655/assets\n",
            "792/792 [==============================] - 25s 32ms/step - loss: 2.2733\n",
            "Epoch 8/25\n",
            "792/792 [==============================] - ETA: 0s - loss: 2.0197\n",
            "Epoch 00008: loss improved from 2.27327 to 2.01973, saving model to ./王安石-checkpoints-1607977655\n",
            "INFO:tensorflow:Assets written to: ./王安石-checkpoints-1607977655/assets\n",
            "792/792 [==============================] - 25s 31ms/step - loss: 2.0197\n",
            "Epoch 9/25\n",
            "791/792 [============================>.] - ETA: 0s - loss: 1.8457\n",
            "Epoch 00009: loss improved from 2.01973 to 1.84579, saving model to ./王安石-checkpoints-1607977655\n",
            "INFO:tensorflow:Assets written to: ./王安石-checkpoints-1607977655/assets\n",
            "792/792 [==============================] - 25s 31ms/step - loss: 1.8458\n",
            "Epoch 10/25\n",
            "792/792 [==============================] - ETA: 0s - loss: 1.7251\n",
            "Epoch 00010: loss improved from 1.84579 to 1.72505, saving model to ./王安石-checkpoints-1607977655\n",
            "INFO:tensorflow:Assets written to: ./王安石-checkpoints-1607977655/assets\n",
            "792/792 [==============================] - 26s 32ms/step - loss: 1.7251\n",
            "Epoch 11/25\n",
            "792/792 [==============================] - ETA: 0s - loss: 1.6397\n",
            "Epoch 00011: loss improved from 1.72505 to 1.63968, saving model to ./王安石-checkpoints-1607977655\n",
            "INFO:tensorflow:Assets written to: ./王安石-checkpoints-1607977655/assets\n",
            "792/792 [==============================] - 25s 32ms/step - loss: 1.6397\n",
            "Epoch 12/25\n",
            "791/792 [============================>.] - ETA: 0s - loss: 1.5779\n",
            "Epoch 00012: loss improved from 1.63968 to 1.57791, saving model to ./王安石-checkpoints-1607977655\n",
            "INFO:tensorflow:Assets written to: ./王安石-checkpoints-1607977655/assets\n",
            "792/792 [==============================] - 25s 31ms/step - loss: 1.5779\n",
            "Epoch 13/25\n",
            "791/792 [============================>.] - ETA: 0s - loss: 1.5314\n",
            "Epoch 00013: loss improved from 1.57791 to 1.53130, saving model to ./王安石-checkpoints-1607977655\n",
            "INFO:tensorflow:Assets written to: ./王安石-checkpoints-1607977655/assets\n",
            "792/792 [==============================] - 25s 32ms/step - loss: 1.5313\n",
            "Epoch 14/25\n",
            "792/792 [==============================] - ETA: 0s - loss: 1.4998\n",
            "Epoch 00014: loss improved from 1.53130 to 1.49982, saving model to ./王安石-checkpoints-1607977655\n",
            "INFO:tensorflow:Assets written to: ./王安石-checkpoints-1607977655/assets\n",
            "792/792 [==============================] - 25s 32ms/step - loss: 1.4998\n",
            "Epoch 15/25\n",
            "791/792 [============================>.] - ETA: 0s - loss: 1.4733\n",
            "Epoch 00015: loss improved from 1.49982 to 1.47338, saving model to ./王安石-checkpoints-1607977655\n",
            "INFO:tensorflow:Assets written to: ./王安石-checkpoints-1607977655/assets\n",
            "792/792 [==============================] - 25s 32ms/step - loss: 1.4734\n",
            "Epoch 16/25\n",
            "792/792 [==============================] - ETA: 0s - loss: 1.4518\n",
            "Epoch 00016: loss improved from 1.47338 to 1.45179, saving model to ./王安石-checkpoints-1607977655\n",
            "INFO:tensorflow:Assets written to: ./王安石-checkpoints-1607977655/assets\n",
            "792/792 [==============================] - 25s 32ms/step - loss: 1.4518\n",
            "Epoch 17/25\n",
            "791/792 [============================>.] - ETA: 0s - loss: 1.4392\n",
            "Epoch 00017: loss improved from 1.45179 to 1.43920, saving model to ./王安石-checkpoints-1607977655\n",
            "INFO:tensorflow:Assets written to: ./王安石-checkpoints-1607977655/assets\n",
            "792/792 [==============================] - 25s 32ms/step - loss: 1.4392\n",
            "Epoch 18/25\n",
            "790/792 [============================>.] - ETA: 0s - loss: 1.4216\n",
            "Epoch 00018: loss improved from 1.43920 to 1.42186, saving model to ./王安石-checkpoints-1607977655\n",
            "INFO:tensorflow:Assets written to: ./王安石-checkpoints-1607977655/assets\n",
            "792/792 [==============================] - 25s 31ms/step - loss: 1.4219\n",
            "Epoch 19/25\n",
            "792/792 [==============================] - ETA: 0s - loss: 1.4121\n",
            "Epoch 00019: loss improved from 1.42186 to 1.41212, saving model to ./王安石-checkpoints-1607977655\n",
            "INFO:tensorflow:Assets written to: ./王安石-checkpoints-1607977655/assets\n",
            "792/792 [==============================] - 25s 32ms/step - loss: 1.4121\n",
            "Epoch 20/25\n",
            "791/792 [============================>.] - ETA: 0s - loss: 1.3984\n",
            "Epoch 00020: loss improved from 1.41212 to 1.39844, saving model to ./王安石-checkpoints-1607977655\n",
            "INFO:tensorflow:Assets written to: ./王安石-checkpoints-1607977655/assets\n",
            "792/792 [==============================] - 25s 31ms/step - loss: 1.3984\n",
            "Epoch 21/25\n",
            "791/792 [============================>.] - ETA: 0s - loss: 1.3915\n",
            "Epoch 00021: loss improved from 1.39844 to 1.39162, saving model to ./王安石-checkpoints-1607977655\n",
            "INFO:tensorflow:Assets written to: ./王安石-checkpoints-1607977655/assets\n",
            "792/792 [==============================] - 25s 32ms/step - loss: 1.3916\n",
            "Epoch 22/25\n",
            "792/792 [==============================] - ETA: 0s - loss: 1.3836\n",
            "Epoch 00022: loss improved from 1.39162 to 1.38358, saving model to ./王安石-checkpoints-1607977655\n",
            "INFO:tensorflow:Assets written to: ./王安石-checkpoints-1607977655/assets\n",
            "792/792 [==============================] - 25s 31ms/step - loss: 1.3836\n",
            "Epoch 23/25\n",
            "792/792 [==============================] - ETA: 0s - loss: 1.3729\n",
            "Epoch 00023: loss improved from 1.38358 to 1.37291, saving model to ./王安石-checkpoints-1607977655\n",
            "INFO:tensorflow:Assets written to: ./王安石-checkpoints-1607977655/assets\n",
            "792/792 [==============================] - 25s 32ms/step - loss: 1.3729\n",
            "Epoch 24/25\n",
            "790/792 [============================>.] - ETA: 0s - loss: 1.3678\n",
            "Epoch 00024: loss improved from 1.37291 to 1.36765, saving model to ./王安石-checkpoints-1607977655\n",
            "INFO:tensorflow:Assets written to: ./王安石-checkpoints-1607977655/assets\n",
            "792/792 [==============================] - 25s 31ms/step - loss: 1.3676\n",
            "Epoch 25/25\n",
            "792/792 [==============================] - ETA: 0s - loss: 1.3605\n",
            "Epoch 00025: loss improved from 1.36765 to 1.36053, saving model to ./王安石-checkpoints-1607977655\n",
            "INFO:tensorflow:Assets written to: ./王安石-checkpoints-1607977655/assets\n",
            "792/792 [==============================] - 25s 32ms/step - loss: 1.3605\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc45044bfd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iy7Z3wvNDkWw"
      },
      "source": [
        "import random\n",
        "import re\n",
        "from keras.models import load_model\n",
        "# training_model = make_model()\n",
        "# training_model.load_weights(filepath)\n",
        "def gen_poem(seed_text):\n",
        "    rows = 4 #输出多少句\n",
        "    cols = 8 #每行多少字，如果五言就6，七言就8\n",
        "    chars = re.findall('[\\x80-\\xff]{3}|[\\w\\W]', seed_text)\n",
        "    if len(chars) != seq_len:\n",
        "        return \"\"\n",
        "    arr = [word_index_dict[k] for k in chars]\n",
        "    for i in range(seq_len, rows * cols):\n",
        "        if (i+1) % cols == 0:\n",
        "            if (i+1) / cols == 2 or (i+1) / cols == 4:\n",
        "                arr.append(1)\n",
        "            else:\n",
        "                arr.append(2)\n",
        "        else:\n",
        "            proba = training_model.predict(np.array(arr[-seq_len:]), verbose=0)\n",
        "            predicted = np.argsort(proba[1])[-5:]\n",
        "            index = random.randint(0,len(predicted)-1)\n",
        "            new_char = predicted[index]\n",
        "            while new_char == 1 or new_char == 2:\n",
        "                index = random.randint(0,len(predicted)-1)\n",
        "                new_char = predicted[index]\n",
        "            arr.append(new_char)\n",
        "    poem = [index_dict[i] for i in arr]\n",
        "    return \"\".join(poem)\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9ox35-XUET0D",
        "outputId": "5b2c8fbf-7146-411d-c6c9-78ca7af24956"
      },
      "source": [
        "gen_poem(\"江南\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'江南望尘生意欲，往与天日西来无。如水中客至因寻，欲归去梨花开小。'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeTo9M1fnIR8"
      },
      "source": [
        ""
      ],
      "execution_count": 62,
      "outputs": []
    }
  ]
}